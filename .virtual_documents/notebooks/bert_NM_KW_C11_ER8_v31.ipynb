


import os
import sys

home_path = os.getcwd().split("DialFill-DM")[0]
task_type = os.getcwd().split("/")[-1]
work_type = os.getcwd().split("/")[-2]
target_path = home_path + "DialFill-DM"
if target_path not in sys.path:
    sys.path.append(target_path)

print(sys.path)

from huggingface_hub import login

login(token="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")

os.environ["CUDA_VISIBLE_DEVICES"] = "0"
os.environ["TOKENIZERS_PARALLELISM"] = "False"
# os.environ["HF_HOME"] = "/data5/qxue/cache/hf_cache/"
# training args
exp_name = "bert_NM_KW_C11_ER8_v31"
DATA_NAME = 'wizard' if 'WOW' in exp_name else 'opendialkg'
args_string = " ".join(
    [
        f"--output_dir {home_path}MyResearch/experiments/{work_type}/{task_type}/{exp_name}",
        (
            ("--input_triples_num 1 --extract_triples_num 3 --retri_triples_num 2 --nega_triples_num 1 --input_kg_from target_kg " +
             "--include_render True --include_triples False --truncate_len 320")
            if DATA_NAME == "wizard" else
            ("--input_triples_num 5 --extract_triples_num 5 --retri_triples_num 3 --nega_triples_num 2 --input_kg_from retri_nega_kg")
        ),
        "--model_name_or_path bert-base-uncased --model_mode token-classification --learning_rate 6e-5 --learning_type DirectCL --only_cls_learning True",

        "--eval_batch_size 8 --do_generate --gpus 1 --num_workers 5 --fp16",
        "--train_batch_size 8 --do_train --max_epochs 10 --patience 1",
        "--max_history 3 --pad_to_multiple_of 4 --num_beams 1 --max_length 50 --type_matter True",

        "--train_tasks_decay 0.0 0.0 0.0",
        "--eval_tasks LM --train_tasks LM --train_tasks_ratio 1.0 --ratio 0.8",
        "--unmask_min_length 2 --unmask_max_length 6",
        "--datasets_mix_type concat",

        "--robust_type Mask --bad_tok_ratio 0.5",
        "--include_prompt False --is_mask_learning False",
        # "--do_eval True",
        # "--is_debug True",
    ]
)
work_type, task_type, exp_name


from datasets import load_dataset
dataset = load_dataset("asnower/opendialkg")


import warnings
warnings.filterwarnings("ignore", message="A parameter name that contains")





import logging
import os
import shutil
import math
import numpy as np
import json

from argparse import Namespace
from dataclasses import dataclass, field, fields
from pathlib import Path
from typing import List, Optional, Union

import pytorch_lightning as pl
import torch
import torch.nn.functional as F

from torch import Tensor
from pytorch_lightning import Trainer
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.callbacks import ModelSummary
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
from transformers import (
    HfArgumentParser,
    AutoTokenizer,
    PreTrainedTokenizer,
)

from dataset_process.utils import ConcatenatedDataset, MixedDataset
from dataset_process.dial_filling import (
    MMI_TASKS,
    GenerationState,
    ExtendedSpecialTokens,
    AugmentDataArguments,
    DialogueFillingDataset,
    Collator as LMCollator,
)

from dataset_process.base_conv import (
    BaseDataArguments,
)
from training.lightning_LLM import (
    BaseTransformer,
    GenerationArguments,
    GenericArguments,
    ModelArguments,
    PeftArguments,
)
from utils.file import extract_split_keyword
from utils.others import compress_repeats


logger = logging.getLogger("run_LM_task")
logging.basicConfig(level=logging.INFO)


TRAIN_TASK = ["LM"]
EVAL_TASK = ["LM"]
_SPLIT_TOKENS = (
    "train",
    "test_seen",
    "test_unseen",
    "valid_seen",
    "valid_unseen",
)


class DialogueTransformer(BaseTransformer):
    mode = "dialogue"

    def __init__(self, hparams: Namespace):
        super().__init__(hparams)

        base_data_fields = {field.name for field in fields(BaseDataArguments)}
        augment_data_fields = {field.name for field in fields(AugmentDataArguments)}
        self.specific_dataset_config = {k: v for k, v in vars(hparams).items() if k in augment_data_fields}
        self.base_dataset_config = {k: v for k, v in vars(hparams).items() if k in base_data_fields}
        self.generation_state = GenerationState()

        if not self.tokenizer.chat_template:
            eos_token = self.tokenizer.eos_token
            start_of_turn = self.tokenizer.eos_token
            end_of_turn = self.tokenizer.eos_token
            new_line = "\n"  # 定义换行符为一个变量，避免在f-string中直接使用反斜杠
            self.tokenizer.chat_template = f"""{"{{ '"+ eos_token + "' }}"}{{% for message in messages %}}{{% if (message['role'] == 'assistant') %}}{{% set role = 'model' %}}{{% else %}}{{% set role = message['role'] %}}{{% endif %}}{"{{ '"+ start_of_turn + "'"} + { "role + '" + new_line + "' +"} message['content'] | trim {"+ '"+ end_of_turn + new_line + "' }}"}{{% endfor %}}{{% if add_generation_prompt %}}{"{{'" + start_of_turn }model{new_line+ "'}}"}{{% endif %}}"""

    def _init_tokenizer(self, tok: Optional[PreTrainedTokenizer]) -> PreTrainedTokenizer:
        if tok is not None:
            return tok

        tokenizer = AutoTokenizer.from_pretrained(
            self.hparams.tokenizer_name or self.hparams.model_name_or_path,
            cache_dir=self.cache_dir,
        )
        self.special_tokens = ExtendedSpecialTokens(tokenizer)

        if tokenizer.eos_token is None:
            tokenizer.eos_token      = tokenizer.sep_token
            tokenizer.eos_token_id   = tokenizer.sep_token_id

        # guarantee pad‑token existence
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        return tokenizer

    def _init_model(self, model):
        if self.hparams.model_mode == "token-classification":
            self.config.num_labels = getattr(self.hparams, "num_labels", 3)
            self.config.id2label = {0: "OTHER", 1: "GOOD", 2: "BAD"}
            self.config.label2id = {v: k for k, v in self.config.id2label.items()}

        return super()._init_model(model)

    def _resize_embeddings_if_needed(self) -> None:
        pass

    def setup(self, stage: str):
        if stage == "fit":
            self.get_dataset("train", self.hparams.train_dataset_path)

    def print_dataset_example(self, dataset, dataset_name):
        for idx in range(1, 3):
            print(f"\033[1;30mExample {idx} of \033[0m \033[1;31m {dataset_name}\033[0m:")
            example = dataset[idx]
            print("-"*50)
            for k, v in example.items():
                if any(num < 0 for num in v) or all(num in range(-2, 3) for num in v):
                    v = compress_repeats(v, repeat_num=10)
                    print(f"\033[1;32m {k} \033[0m:{v}")
                else:
                    print(f"\033[1;32m {k} \033[0m:{self.tokenizer.decode(v)}")
            print('\n')

    def get_dataset(self, mode: str, data_path: str, **kwargs):
        if mode == "test":
            dataset_path = self.hparams.test_dataset_path
            self.test_dataset = self.get_special_dataset(dataset_path, 'lm', is_generation=True)
            return self.test_dataset
        elif mode == "valid":
            dataset_path = self.hparams.eval_dataset_path
            eval_task = [t for t in self.hparams.eval_tasks if t in EVAL_TASK]
            self.eval_dataset = self.get_special_dataset(dataset_path, eval_task[0])
            return self.eval_dataset
        else:
            dataset_path = self.hparams.train_dataset_path
            train_task = [t for t in self.hparams.train_tasks if t in TRAIN_TASK]
            self.train_datasets = {}
            for task in train_task:
                self.train_datasets[task] = self.get_special_dataset(dataset_path, task)
                self.print_dataset_example(self.train_datasets[task], f"task {task} train dataset")

            datasets = [*list(self.train_datasets.values())]
            ratios = self.hparams.train_tasks_ratio.copy()
            decays = []
            for decay in self.hparams.train_tasks_decay:
                decays.append(math.exp(-decay * self.current_epoch))
            ratios = [round(x*y, 2) for x, y in zip(ratios, decays)]
            logger.warning(f"{ratios=} {decays=} {self.current_epoch=}")

            mix_type = self.hparams.datasets_mix_type.lower()
            if mix_type == "mix":
                self.total_dataset = MixedDataset(list(zip(datasets, ratios)))
            elif mix_type == "concat":
                self.total_dataset = ConcatenatedDataset(list(zip(datasets, ratios)))
            else:
                raise ValueError(f"Unknown datasets_mix_type: {mix_type}")

            return self.total_dataset

    def get_special_dataset(self, dataset_path, task, is_generation=False):
        self.muti_output = False
        retrieved_kg = self.get_retrieved_kg(
            extract_split_keyword(dataset_path, _SPLIT_TOKENS),
        )

        return DialogueFillingDataset(
            dataset_path=dataset_path,
            tokenizer=self.tokenizer,
            train_task_type=task,
            is_generation=is_generation,
            retrieved_kg=retrieved_kg,
            muti_output=self.muti_output,
            special_tokens=self.special_tokens,
            generation_state=self.generation_state,
            **self.specific_dataset_config,
            **self.base_dataset_config,
        )

    def get_collator(self, mode: str):
        padding_side = 'left'
        logger.info(f"padding_side is set to {padding_side}")
        return LMCollator(
            self.tokenizer.pad_token_id,
            kg_pad=None,
            as_tuple=False,
            pad_to_multiple_of=self.hparams.pad_to_multiple_of,
            padding_side=padding_side,
            truncate_len=self.hparams.truncate_len,
        )

    def train_dataloader(self) -> DataLoader:
        if self.current_epoch >= self.hparams.num_lm_warmup_epochs:
            dataloader = self.get_dataloader(
                "train",
                self.hparams.train_batch_size,
                shuffle=True,
                dataset=self.total_dataset,
            )
            for example in dataloader:
                break
            print("Random Example of dataloader:")
            print("-"*50)
            for k, v in example.items():
                if k not in ['attention_mask']:
                    print(f"{k}[0]:{v[0]}")
                    print(f"{k}[1]:{v[1]}")
            return dataloader
        else:
            return DataLoader(
                self.train_datasets["lm"],
                batch_size=self.hparams.train_batch_size,
                shuffle=True,
                collate_fn=self.get_collator("train"),
                num_workers=self.hparams.num_workers,
                pin_memory=True,
            )

    # ---------- 1. 帮助函数：把 signs → token-cls 标签 ----------
    def _signs_to_cls_labels(self, signs: torch.Tensor) -> torch.Tensor:
        labels = torch.full_like(signs, -100)
        labels[(signs == 1) | (signs == 2)]   = 1
        labels[(signs == -1) | (signs == -2)] = 2
        return labels

    def training_step(self, raw_batch, batch_nb):
        # GPT 路径（保持原样）
        if self.hparams.model_mode == "token-classification":
            # -------- BERT token-classification 路径 --------
            # 1) 把 signs 改成 labels，去掉 GPT 特有的字段
            signs = raw_batch.pop("signs")                 # Tensor [B, L]
            raw_batch["labels"] = self._signs_to_cls_labels(signs)

            inputs_to_keep = {'input_ids', 'attention_mask', 'labels'}
            filtered_inputs = {k: raw_batch[k] for k in inputs_to_keep if k in raw_batch}
            outputs = self.model(**filtered_inputs)
            loss = outputs.loss

            # 3) metric：token-level accuracy（简单示例，按需替换）
            with torch.no_grad():
                preds = outputs.logits.argmax(-1)
                mask  = raw_batch["labels"] != -100
                acc   = (preds[mask] == raw_batch["labels"][mask]).float().mean()
            self.log_dict(
                {"train/loss": loss, "train/acc": acc},
                prog_bar=True, logger=True
            )
            return loss
        else:
            return super().training_step(raw_batch, batch_nb)

    @torch.no_grad()
    def validation_step(self, raw_batch, batch_nb):
        if self.hparams.model_mode == "language-modeling":
            return super().validation_step(raw_batch, batch_nb)

        signs = raw_batch.pop("signs")
        raw_batch["labels"] = self._signs_to_cls_labels(signs)
        inputs_to_keep = {'input_ids', 'attention_mask', 'labels'}
        filtered_inputs = {k: raw_batch[k] for k in inputs_to_keep if k in raw_batch}
        outputs = self.model(**filtered_inputs)
        loss = outputs.loss

        preds = outputs.logits.argmax(-1)
        mask  = raw_batch["labels"] != -100
        acc   = (preds[mask] == raw_batch["labels"][mask]).float().mean()

        # 收集到 self.val_output_list，后面 on_validation_epoch_end 里求平均
        self.val_output_list.append(
            {"val_loss": loss.detach(), "val_acc": acc.detach()}
        )

    def on_validation_epoch_end(self):
        if self.hparams.model_mode == "language-modeling":
            return super().on_validation_epoch_end()

        loss = torch.stack([x["val_loss"] for x in self.val_output_list]).mean()
        acc  = torch.stack([x["val_acc"]  for x in self.val_output_list]).mean()
        self.log_dict({"valid/loss": loss, "valid/acc": acc}, prog_bar=True)

    def on_test_epoch_start(self) -> None:
        super().on_test_epoch_start()
        self.test_output_list = []
        return

    @torch.no_grad()
    def test_step(self, raw_batch, batch_no):
        mode = self.generation_state.mode 


@dataclass
class SpecificArguments:
    mix_tasks: List[str] = field(
        default_factory=lambda: [],
        metadata={"choices": [*TRAIN_TASK]}
    )
    chosen_knowledge_topk: int = 5
    datasets_mix_type: str = field(
        default="concat",
        metadata={"choices": ["concat", "mix", "mix_then_concat"]},
    )
    train_tasks: List[str] = field(
        default_factory=lambda: ["LM"],
        metadata={"choices": [*TRAIN_TASK]}
    )
    train_tasks_ratio: List[float] = field(
        default_factory=lambda: [1.0],
    )
    train_tasks_decay: List[float] = field(
        default_factory=lambda: [0.0],
    )
    eval_tasks: List[str] = field(
        default_factory=lambda: ["LM"],
        metadata={"choices": [*TRAIN_TASK]}
    )
    knowledge_to_triple_path: str = None
    chosen_knowledge_path: str = None
    generation_process: List[str] = field(
        default_factory=lambda: ["response"],
        metadata={"choices": ["revise", "keywords", "pre_masked", "post_masked", "response"]}
    )


def _validate(args):
    if args.do_generate:
        if args.test_dataset_path is None:
            raise ValueError("`--test_dataset_path` is required when `--do_generate` is set")





parser = HfArgumentParser(
    (
        GenerationArguments,
        BaseDataArguments,
        ModelArguments,
        GenericArguments,
        PeftArguments,
        SpecificArguments,
        AugmentDataArguments,
    )
)

args = parser.parse_args(args_string.split())
_validate(args)

odir = Path(args.output_dir)
if args.overwrite_output_dir and odir.exists():
    shutil.rmtree(odir)
odir.mkdir(parents=True, exist_ok=True)


pl_logger = pl_loggers.TensorBoardLogger(
    save_dir=args.output_dir,
    name="train_logs" if args.do_train else "valid_logs",
    default_hp_metric=False,
)


def get_trainer(args):
    extra_callbacks = []
    if args.do_train and args.patience > 0:
        extra_callbacks.append(pl.callbacks.EarlyStopping(monitor="valid/loss"))

    checkpoint_callback = pl.callbacks.ModelCheckpoint(
        dirpath=args.output_dir,
        monitor="valid/loss",
        mode="min",
        save_top_k=1,
    )

    trainer_kwargs = {
        "reload_dataloaders_every_n_epochs": 1,
        "max_epochs": args.max_epochs,
    }
    extra_callbacks = extra_callbacks or []
    pl.seed_everything(args.seed)

    train_params = trainer_kwargs or {}

    if args.fp16:
        train_params["precision"] = "16-mixed"
    if args.bf16:
        train_params["precision"] = "bf16-mixed"

    if args.gpus > 1 or (args.gpus == -1 and torch.cuda.device_count() > 1):
        train_params["distributed_backend"] = "ddp"

    train_params["accumulate_grad_batches"] = args.accumulate_grad_batches
    return Trainer(
        callbacks=[ModelSummary(max_depth=1), checkpoint_callback] + extra_callbacks,
        logger=pl_logger,
        **train_params,
    )


try:
    if args.do_train:
        model = DialogueTransformer(args)
        trainer = get_trainer(args)
        trainer.fit(model)
except:
    # requests.get(f"https://api.day.app/39JhrJNt6ChamFHTYpHAx7/模型训练有bug/您的训练模型{exp_name}代码未完成")
    assert 0





import re

process_dict = {
    # Step 1
    'Ke': ['keyword_extract'],
    'Kg': ['keyword_generate'],
    'Krv': ['retrieved_keywords_valid'],
    'Kgv': ['retrieved_knowledges_valid'],
    'Kgrv': ['retrieved_kwd_and_kg_valid'],
    'Kagv': ['retrieved_kwd_as_kg_valid'],
    # Step 2
    'Mg': ['mask_pre_generate', 'mask_post_generate'],
    'Mra': ['mask_random_append'],
    'Mv': ['mask_pre_valid', 'mask_post_valid'],
    # Step 3
    'Rf': ['response_filling'],
}

new_generated_name = []
if args.do_generate:
    args.retri_triples_num = args.input_triples_num
    args.extract_kg_from = args.input_kg_from = 'retrieved_kg'
    if os.path.exists(str(args.output_dir) + "/best_model/lm"):
        best_model_path = str(args.output_dir) + "/best_model/lm"
    if args.peft_train:
        args.peft_pretrain_model = True
        args.adapter_name_or_path = best_model_path
    else:
        args.model_name_or_path = best_model_path

    args.add_mask_min_length = mmin = 5
    args.add_mask_max_length = mmax = 10

    trainer = get_trainer(args)
    for num_beams in [1]:
        args.num_beams = num_beams
        for split in [
            'seen',
            # 'unseen'
        ]:
            args.chosen_knowledge_path = f"{home_path}MyResearch/notebooks/experiments/related_works/GATE/{DATA_NAME}_test_{split}_top10_kg.jsonl"
            args.test_dataset_path = f"{home_path}MyResearch/data/processed/{DATA_NAME}/Topic_LLM/test_{split}.jsonl"
            for key in [
                "KrvMv",
            ]:
                process_list = [
                    handler
                    for seg in re.findall(r'[A-Z][^A-Z]*', key)
                    for handler in process_dict.get(seg, [''])
                ]
                assert '' not in process_list
                args.generation_name = f'output_test_{split}_{key}_m{mmin}-m{mmax}'
                new_generated_name.append(args.generation_name)
                model = DialogueTransformer(args)
                for process in process_list:
                    if process in ['keyword_extract', 'mask_random_append']:
                        continue
                    print(f'{process} is now processing!')
                    model.generation_state.mode = process
                    trainer.test(model)
                    model.generation_state.print_state()





if not args.do_eval:
    assert 0
