{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e011277",
   "metadata": {},
   "source": [
    "# args setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17751855",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/xueqiang/anaconda3/envs/NPH/lib/python39.zip', '/home/xueqiang/anaconda3/envs/NPH/lib/python3.9', '/home/xueqiang/anaconda3/envs/NPH/lib/python3.9/lib-dynload', '', '/home/xueqiang/anaconda3/envs/NPH/lib/python3.9/site-packages', '/home/xueqiang/MyResearch']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('my_works', 'DialFill_v3', 'bert_NM_KW_C11_ER8_v31')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "home_path = os.getcwd().split(\"DialFill-DM\")[0]\n",
    "task_type = os.getcwd().split(\"/\")[-1]\n",
    "work_type = os.getcwd().split(\"/\")[-2]\n",
    "target_path = home_path + \"DialFill-DM\"\n",
    "if target_path not in sys.path:\n",
    "    sys.path.append(target_path)\n",
    "\n",
    "print(sys.path)\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "# os.environ[\"HF_HOME\"] = \"/data5/qxue/cache/hf_cache/\"\n",
    "# training args\n",
    "exp_name = \"bert_NM_KW_C11_ER8_v31\"\n",
    "DATA_NAME = 'wizard' if 'WOW' in exp_name else 'opendialkg'\n",
    "args_string = \" \".join(\n",
    "    [\n",
    "        f\"--output_dir {home_path}MyResearch/experiments/{work_type}/{task_type}/{exp_name}\",\n",
    "        (\n",
    "            (\"--input_triples_num 1 --extract_triples_num 3 --retri_triples_num 2 --nega_triples_num 1 --input_kg_from target_kg \" +\n",
    "             \"--include_render True --include_triples False --truncate_len 320\")\n",
    "            if DATA_NAME == \"wizard\" else\n",
    "            (\"--input_triples_num 5 --extract_triples_num 5 --retri_triples_num 3 --nega_triples_num 2 --input_kg_from retri_nega_kg\")\n",
    "        ),\n",
    "        \"--model_name_or_path bert-base-uncased --model_mode token-classification --learning_rate 6e-5 --learning_type DirectCL --only_cls_learning True\",\n",
    "\n",
    "        \"--eval_batch_size 8 --do_generate --gpus 1 --num_workers 5 --fp16\",\n",
    "        \"--train_batch_size 8 --do_train --max_epochs 10 --patience 1\",\n",
    "        \"--max_history 3 --pad_to_multiple_of 4 --num_beams 1 --max_length 50 --type_matter True\",\n",
    "\n",
    "        \"--train_tasks_decay 0.0 0.0 0.0\",\n",
    "        \"--eval_tasks LM --train_tasks LM --train_tasks_ratio 1.0 --ratio 0.8\",\n",
    "        \"--unmask_min_length 2 --unmask_max_length 6\",\n",
    "        \"--datasets_mix_type concat\",\n",
    "\n",
    "        \"--robust_type Mask --bad_tok_ratio 0.5\",\n",
    "        \"--include_prompt False --is_mask_learning False\",\n",
    "        # \"--do_eval True\",\n",
    "        # \"--is_debug True\",\n",
    "    ]\n",
    ")\n",
    "work_type, task_type, exp_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec6c047-7014-4057-97d3-16954eb5a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"asnower/opendialkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c091e1-e2d0-4146-8ec4-bb1fafb008d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e8b03b",
   "metadata": {},
   "source": [
    "# class determinetion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66681c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from argparse import Namespace\n",
    "from dataclasses import dataclass, field, fields\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import ModelSummary\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizer,\n",
    ")\n",
    "\n",
    "from dataset_process.utils import ConcatenatedDataset, MixedDataset\n",
    "from dataset_process.dial_filling import (\n",
    "    MMI_TASKS,\n",
    "    GenerationState,\n",
    "    ExtendedSpecialTokens,\n",
    "    AugmentDataArguments,\n",
    "    DialogueFillingDataset,\n",
    "    Collator as LMCollator,\n",
    ")\n",
    "\n",
    "from dataset_process.base_conv import (\n",
    "    BaseDataArguments,\n",
    ")\n",
    "from training.lightning_LLM import (\n",
    "    BaseTransformer,\n",
    "    GenerationArguments,\n",
    "    GenericArguments,\n",
    "    ModelArguments,\n",
    "    PeftArguments,\n",
    ")\n",
    "from utils.file import extract_split_keyword\n",
    "from utils.others import compress_repeats\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"run_LM_task\")\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a29051f-9616-4178-92c5-90ad080f32c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_TASK = [\"LM\"]\n",
    "EVAL_TASK = [\"LM\"]\n",
    "_SPLIT_TOKENS = (\n",
    "    \"train\",\n",
    "    \"test_seen\",\n",
    "    \"test_unseen\",\n",
    "    \"valid_seen\",\n",
    "    \"valid_unseen\",\n",
    ")\n",
    "\n",
    "\n",
    "class DialogueTransformer(BaseTransformer):\n",
    "    mode = \"dialogue\"\n",
    "\n",
    "    def __init__(self, hparams: Namespace):\n",
    "        super().__init__(hparams)\n",
    "\n",
    "        base_data_fields = {field.name for field in fields(BaseDataArguments)}\n",
    "        augment_data_fields = {field.name for field in fields(AugmentDataArguments)}\n",
    "        self.specific_dataset_config = {k: v for k, v in vars(hparams).items() if k in augment_data_fields}\n",
    "        self.base_dataset_config = {k: v for k, v in vars(hparams).items() if k in base_data_fields}\n",
    "        self.generation_state = GenerationState()\n",
    "\n",
    "        if not self.tokenizer.chat_template:\n",
    "            eos_token = self.tokenizer.eos_token\n",
    "            start_of_turn = self.tokenizer.eos_token\n",
    "            end_of_turn = self.tokenizer.eos_token\n",
    "            new_line = \"\\n\"  # 定义换行符为一个变量，避免在f-string中直接使用反斜杠\n",
    "            self.tokenizer.chat_template = f\"\"\"{\"{{ '\"+ eos_token + \"' }}\"}{{% for message in messages %}}{{% if (message['role'] == 'assistant') %}}{{% set role = 'model' %}}{{% else %}}{{% set role = message['role'] %}}{{% endif %}}{\"{{ '\"+ start_of_turn + \"'\"} + { \"role + '\" + new_line + \"' +\"} message['content'] | trim {\"+ '\"+ end_of_turn + new_line + \"' }}\"}{{% endfor %}}{{% if add_generation_prompt %}}{\"{{'\" + start_of_turn }model{new_line+ \"'}}\"}{{% endif %}}\"\"\"\n",
    "\n",
    "    def _init_tokenizer(self, tok: Optional[PreTrainedTokenizer]) -> PreTrainedTokenizer:\n",
    "        if tok is not None:\n",
    "            return tok\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.hparams.tokenizer_name or self.hparams.model_name_or_path,\n",
    "            cache_dir=self.cache_dir,\n",
    "        )\n",
    "        self.special_tokens = ExtendedSpecialTokens(tokenizer)\n",
    "\n",
    "        if tokenizer.eos_token is None:\n",
    "            tokenizer.eos_token      = tokenizer.sep_token\n",
    "            tokenizer.eos_token_id   = tokenizer.sep_token_id\n",
    "\n",
    "        # guarantee pad‑token existence\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def _init_model(self, model):\n",
    "        if self.hparams.model_mode == \"token-classification\":\n",
    "            self.config.num_labels = getattr(self.hparams, \"num_labels\", 3)\n",
    "            self.config.id2label = {0: \"OTHER\", 1: \"GOOD\", 2: \"BAD\"}\n",
    "            self.config.label2id = {v: k for k, v in self.config.id2label.items()}\n",
    "\n",
    "        return super()._init_model(model)\n",
    "\n",
    "    def _resize_embeddings_if_needed(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        if stage == \"fit\":\n",
    "            self.get_dataset(\"train\", self.hparams.train_dataset_path)\n",
    "\n",
    "    def print_dataset_example(self, dataset, dataset_name):\n",
    "        for idx in range(1, 3):\n",
    "            print(f\"\\033[1;30mExample {idx} of \\033[0m \\033[1;31m {dataset_name}\\033[0m:\")\n",
    "            example = dataset[idx]\n",
    "            print(\"-\"*50)\n",
    "            for k, v in example.items():\n",
    "                if any(num < 0 for num in v) or all(num in range(-2, 3) for num in v):\n",
    "                    v = compress_repeats(v, repeat_num=10)\n",
    "                    print(f\"\\033[1;32m {k} \\033[0m:{v}\")\n",
    "                else:\n",
    "                    print(f\"\\033[1;32m {k} \\033[0m:{self.tokenizer.decode(v)}\")\n",
    "            print('\\n')\n",
    "\n",
    "    def get_dataset(self, mode: str, data_path: str, **kwargs):\n",
    "        if mode == \"test\":\n",
    "            dataset_path = self.hparams.test_dataset_path\n",
    "            self.test_dataset = self.get_special_dataset(dataset_path, 'lm', is_generation=True)\n",
    "            return self.test_dataset\n",
    "        elif mode == \"valid\":\n",
    "            dataset_path = self.hparams.eval_dataset_path\n",
    "            eval_task = [t for t in self.hparams.eval_tasks if t in EVAL_TASK]\n",
    "            self.eval_dataset = self.get_special_dataset(dataset_path, eval_task[0])\n",
    "            return self.eval_dataset\n",
    "        else:\n",
    "            dataset_path = self.hparams.train_dataset_path\n",
    "            train_task = [t for t in self.hparams.train_tasks if t in TRAIN_TASK]\n",
    "            self.train_datasets = {}\n",
    "            for task in train_task:\n",
    "                self.train_datasets[task] = self.get_special_dataset(dataset_path, task)\n",
    "                self.print_dataset_example(self.train_datasets[task], f\"task {task} train dataset\")\n",
    "\n",
    "            datasets = [*list(self.train_datasets.values())]\n",
    "            ratios = self.hparams.train_tasks_ratio.copy()\n",
    "            decays = []\n",
    "            for decay in self.hparams.train_tasks_decay:\n",
    "                decays.append(math.exp(-decay * self.current_epoch))\n",
    "            ratios = [round(x*y, 2) for x, y in zip(ratios, decays)]\n",
    "            logger.warning(f\"{ratios=} {decays=} {self.current_epoch=}\")\n",
    "\n",
    "            mix_type = self.hparams.datasets_mix_type.lower()\n",
    "            if mix_type == \"mix\":\n",
    "                self.total_dataset = MixedDataset(list(zip(datasets, ratios)))\n",
    "            elif mix_type == \"concat\":\n",
    "                self.total_dataset = ConcatenatedDataset(list(zip(datasets, ratios)))\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown datasets_mix_type: {mix_type}\")\n",
    "\n",
    "            return self.total_dataset\n",
    "\n",
    "    def get_special_dataset(self, dataset_path, task, is_generation=False):\n",
    "        self.muti_output = False\n",
    "        retrieved_kg = self.get_retrieved_kg(\n",
    "            extract_split_keyword(dataset_path, _SPLIT_TOKENS),\n",
    "        )\n",
    "\n",
    "        return DialogueFillingDataset(\n",
    "            dataset_path=dataset_path,\n",
    "            tokenizer=self.tokenizer,\n",
    "            train_task_type=task,\n",
    "            is_generation=is_generation,\n",
    "            retrieved_kg=retrieved_kg,\n",
    "            muti_output=self.muti_output,\n",
    "            special_tokens=self.special_tokens,\n",
    "            generation_state=self.generation_state,\n",
    "            **self.specific_dataset_config,\n",
    "            **self.base_dataset_config,\n",
    "        )\n",
    "\n",
    "    def get_collator(self, mode: str):\n",
    "        padding_side = 'left'\n",
    "        logger.info(f\"padding_side is set to {padding_side}\")\n",
    "        return LMCollator(\n",
    "            self.tokenizer.pad_token_id,\n",
    "            kg_pad=None,\n",
    "            as_tuple=False,\n",
    "            pad_to_multiple_of=self.hparams.pad_to_multiple_of,\n",
    "            padding_side=padding_side,\n",
    "            truncate_len=self.hparams.truncate_len,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        if self.current_epoch >= self.hparams.num_lm_warmup_epochs:\n",
    "            dataloader = self.get_dataloader(\n",
    "                \"train\",\n",
    "                self.hparams.train_batch_size,\n",
    "                shuffle=True,\n",
    "                dataset=self.total_dataset,\n",
    "            )\n",
    "            for example in dataloader:\n",
    "                break\n",
    "            print(\"Random Example of dataloader:\")\n",
    "            print(\"-\"*50)\n",
    "            for k, v in example.items():\n",
    "                if k not in ['attention_mask']:\n",
    "                    print(f\"{k}[0]:{v[0]}\")\n",
    "                    print(f\"{k}[1]:{v[1]}\")\n",
    "            return dataloader\n",
    "        else:\n",
    "            return DataLoader(\n",
    "                self.train_datasets[\"lm\"],\n",
    "                batch_size=self.hparams.train_batch_size,\n",
    "                shuffle=True,\n",
    "                collate_fn=self.get_collator(\"train\"),\n",
    "                num_workers=self.hparams.num_workers,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "    # ---------- 1. 帮助函数：把 signs → token-cls 标签 ----------\n",
    "    def _signs_to_cls_labels(self, signs: torch.Tensor) -> torch.Tensor:\n",
    "        labels = torch.full_like(signs, -100)\n",
    "        labels[(signs == 1) | (signs == 2)]   = 1\n",
    "        labels[(signs == -1) | (signs == -2)] = 2\n",
    "        return labels\n",
    "\n",
    "    def training_step(self, raw_batch, batch_nb):\n",
    "        # GPT 路径（保持原样）\n",
    "        if self.hparams.model_mode == \"token-classification\":\n",
    "            # -------- BERT token-classification 路径 --------\n",
    "            # 1) 把 signs 改成 labels，去掉 GPT 特有的字段\n",
    "            signs = raw_batch.pop(\"signs\")                 # Tensor [B, L]\n",
    "            raw_batch[\"labels\"] = self._signs_to_cls_labels(signs)\n",
    "\n",
    "            inputs_to_keep = {'input_ids', 'attention_mask', 'labels'}\n",
    "            filtered_inputs = {k: raw_batch[k] for k in inputs_to_keep if k in raw_batch}\n",
    "            outputs = self.model(**filtered_inputs)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # 3) metric：token-level accuracy（简单示例，按需替换）\n",
    "            with torch.no_grad():\n",
    "                preds = outputs.logits.argmax(-1)\n",
    "                mask  = raw_batch[\"labels\"] != -100\n",
    "                acc   = (preds[mask] == raw_batch[\"labels\"][mask]).float().mean()\n",
    "            self.log_dict(\n",
    "                {\"train/loss\": loss, \"train/acc\": acc},\n",
    "                prog_bar=True, logger=True\n",
    "            )\n",
    "            return loss\n",
    "        else:\n",
    "            return super().training_step(raw_batch, batch_nb)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validation_step(self, raw_batch, batch_nb):\n",
    "        if self.hparams.model_mode == \"language-modeling\":\n",
    "            return super().validation_step(raw_batch, batch_nb)\n",
    "\n",
    "        signs = raw_batch.pop(\"signs\")\n",
    "        raw_batch[\"labels\"] = self._signs_to_cls_labels(signs)\n",
    "        inputs_to_keep = {'input_ids', 'attention_mask', 'labels'}\n",
    "        filtered_inputs = {k: raw_batch[k] for k in inputs_to_keep if k in raw_batch}\n",
    "        outputs = self.model(**filtered_inputs)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        preds = outputs.logits.argmax(-1)\n",
    "        mask  = raw_batch[\"labels\"] != -100\n",
    "        acc   = (preds[mask] == raw_batch[\"labels\"][mask]).float().mean()\n",
    "\n",
    "        # 收集到 self.val_output_list，后面 on_validation_epoch_end 里求平均\n",
    "        self.val_output_list.append(\n",
    "            {\"val_loss\": loss.detach(), \"val_acc\": acc.detach()}\n",
    "        )\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.hparams.model_mode == \"language-modeling\":\n",
    "            return super().on_validation_epoch_end()\n",
    "\n",
    "        loss = torch.stack([x[\"val_loss\"] for x in self.val_output_list]).mean()\n",
    "        acc  = torch.stack([x[\"val_acc\"]  for x in self.val_output_list]).mean()\n",
    "        self.log_dict({\"valid/loss\": loss, \"valid/acc\": acc}, prog_bar=True)\n",
    "\n",
    "    def on_test_epoch_start(self) -> None:\n",
    "        super().on_test_epoch_start()\n",
    "        self.test_output_list = []\n",
    "        return\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test_step(self, raw_batch, batch_no):\n",
    "        mode = self.generation_state.mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a537b7c-9225-4882-a40d-bb2892320f74",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpecificArguments:\n",
    "    mix_tasks: List[str] = field(\n",
    "        default_factory=lambda: [],\n",
    "        metadata={\"choices\": [*TRAIN_TASK]}\n",
    "    )\n",
    "    chosen_knowledge_topk: int = 5\n",
    "    datasets_mix_type: str = field(\n",
    "        default=\"concat\",\n",
    "        metadata={\"choices\": [\"concat\", \"mix\", \"mix_then_concat\"]},\n",
    "    )\n",
    "    train_tasks: List[str] = field(\n",
    "        default_factory=lambda: [\"LM\"],\n",
    "        metadata={\"choices\": [*TRAIN_TASK]}\n",
    "    )\n",
    "    train_tasks_ratio: List[float] = field(\n",
    "        default_factory=lambda: [1.0],\n",
    "    )\n",
    "    train_tasks_decay: List[float] = field(\n",
    "        default_factory=lambda: [0.0],\n",
    "    )\n",
    "    eval_tasks: List[str] = field(\n",
    "        default_factory=lambda: [\"LM\"],\n",
    "        metadata={\"choices\": [*TRAIN_TASK]}\n",
    "    )\n",
    "    knowledge_to_triple_path: str = None\n",
    "    chosen_knowledge_path: str = None\n",
    "    generation_process: List[str] = field(\n",
    "        default_factory=lambda: [\"response\"],\n",
    "        metadata={\"choices\": [\"revise\", \"keywords\", \"pre_masked\", \"post_masked\", \"response\"]}\n",
    "    )\n",
    "\n",
    "\n",
    "def _validate(args):\n",
    "    if args.do_generate:\n",
    "        if args.test_dataset_path is None:\n",
    "            raise ValueError(\"`--test_dataset_path` is required when `--do_generate` is set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83d97b",
   "metadata": {},
   "source": [
    "# trianing or generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23303281",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (\n",
    "        GenerationArguments,\n",
    "        BaseDataArguments,\n",
    "        ModelArguments,\n",
    "        GenericArguments,\n",
    "        PeftArguments,\n",
    "        SpecificArguments,\n",
    "        AugmentDataArguments,\n",
    "    )\n",
    ")\n",
    "\n",
    "args = parser.parse_args(args_string.split())\n",
    "_validate(args)\n",
    "\n",
    "odir = Path(args.output_dir)\n",
    "if args.overwrite_output_dir and odir.exists():\n",
    "    shutil.rmtree(odir)\n",
    "odir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4896c846",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl_logger = pl_loggers.TensorBoardLogger(\n",
    "    save_dir=args.output_dir,\n",
    "    name=\"train_logs\" if args.do_train else \"valid_logs\",\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "\n",
    "\n",
    "def get_trainer(args):\n",
    "    extra_callbacks = []\n",
    "    if args.do_train and args.patience > 0:\n",
    "        extra_callbacks.append(pl.callbacks.EarlyStopping(monitor=\"valid/loss\"))\n",
    "\n",
    "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "        dirpath=args.output_dir,\n",
    "        monitor=\"valid/loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "    )\n",
    "\n",
    "    trainer_kwargs = {\n",
    "        \"reload_dataloaders_every_n_epochs\": 1,\n",
    "        \"max_epochs\": args.max_epochs,\n",
    "    }\n",
    "    extra_callbacks = extra_callbacks or []\n",
    "    pl.seed_everything(args.seed)\n",
    "\n",
    "    train_params = trainer_kwargs or {}\n",
    "\n",
    "    if args.fp16:\n",
    "        train_params[\"precision\"] = \"16-mixed\"\n",
    "    if args.bf16:\n",
    "        train_params[\"precision\"] = \"bf16-mixed\"\n",
    "\n",
    "    if args.gpus > 1 or (args.gpus == -1 and torch.cuda.device_count() > 1):\n",
    "        train_params[\"distributed_backend\"] = \"ddp\"\n",
    "\n",
    "    train_params[\"accumulate_grad_batches\"] = args.accumulate_grad_batches\n",
    "    return Trainer(\n",
    "        callbacks=[ModelSummary(max_depth=1), checkpoint_callback] + extra_callbacks,\n",
    "        logger=pl_logger,\n",
    "        **train_params,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ffd126f-d171-4412-84cb-0f6d41d0bf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if args.do_train:\n",
    "        model = DialogueTransformer(args)\n",
    "        trainer = get_trainer(args)\n",
    "        trainer.fit(model)\n",
    "except:\n",
    "    # requests.get(f\"https://api.day.app/39JhrJNt6ChamFHTYpHAx7/模型训练有bug/您的训练模型{exp_name}代码未完成\")\n",
    "    assert 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbeb9ea-7d6d-4e9d-afa9-b881fc90cd6e",
   "metadata": {},
   "source": [
    "{'count': 72837,\n",
    " 'min': 57,\n",
    " 'max': 457,\n",
    " 'mean': 186.76463885113336,\n",
    " 'median': 184.0,\n",
    " 'std': 42.33490382702918,\n",
    " 'percentiles': {50: 184, 75: 213, 90: 240, 95: 259, 98: 284, 99: 302}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9de3c0f-7142-4dec-a520-80fd400a8f5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:src.training.lightning_LLM:Model loaded from /home/xueqiang/MyResearch/experiments/my_works/DialFill_v3/bert_NM_KW_C11_ER8_v31/best_model/lm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieved_keywords_valid is now processing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 10:50:52.226657: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-15 10:50:52.241875: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-15 10:50:52.246469: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-15 10:50:52.257980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-15 10:50:52.998395: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:conv_data_LLM:#examples with triples 2095 (100.0%) out of 2095 examples\n",
      "INFO:conv_data_LLM:#examples with no triples 0 (0.0%) and incomplete triples 0 (0.0%)\n",
      "INFO:run_LM_task:padding_side is set to right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32e75658d98d40ed81726ea7ba46bb02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:conv_data_LLM:#examples with triples 2095 (100.0%) out of 2095 examples\n",
      "INFO:conv_data_LLM:#examples with no triples 0 (0.0%) and incomplete triples 0 (0.0%)\n",
      "WARNING:run_LM_task:[SAVE] generation / validation results ➜ /home/xueqiang/MyResearch/experiments/my_works/DialFill_v3/bert_NM_KW_C11_ER8_v31/output_test_seen_KrvMv_m5-m10.jsonl\n",
      "INFO:dialogue_copy_data:mode=retrieved_keywords_valid | keywords=2095, keywords_gens=0, keywords_cands=2095, keywords_scores=2095, keywords_is_good=2095, knowledges=0, knowledges_cands=0, knowledges_scores=0, knowledges_is_good=0, masked_pre=0, masked_post=0, masked_pre_scores=0, masked_post_scores=0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_pre_valid is now processing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:conv_data_LLM:#examples with triples 2095 (100.0%) out of 2095 examples\n",
      "INFO:conv_data_LLM:#examples with no triples 0 (0.0%) and incomplete triples 0 (0.0%)\n",
      "INFO:run_LM_task:padding_side is set to right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11749132ef04090a4ecb26b463c6aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:conv_data_LLM:#examples with triples 2095 (100.0%) out of 2095 examples\n",
      "INFO:conv_data_LLM:#examples with no triples 0 (0.0%) and incomplete triples 0 (0.0%)\n",
      "WARNING:run_LM_task:[SAVE] generation / validation results ➜ /home/xueqiang/MyResearch/experiments/my_works/DialFill_v3/bert_NM_KW_C11_ER8_v31/output_test_seen_KrvMv_m5-m10.jsonl\n",
      "INFO:dialogue_copy_data:mode=mask_pre_valid | keywords=2095, keywords_gens=0, keywords_cands=2095, keywords_scores=2095, keywords_is_good=2095, knowledges=0, knowledges_cands=0, knowledges_scores=0, knowledges_is_good=0, masked_pre=2095, masked_post=0, masked_pre_scores=2095, masked_post_scores=0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_post_valid is now processing!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:conv_data_LLM:#examples with triples 2095 (100.0%) out of 2095 examples\n",
      "INFO:conv_data_LLM:#examples with no triples 0 (0.0%) and incomplete triples 0 (0.0%)\n",
      "INFO:run_LM_task:padding_side is set to right\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81074810141f4e0cb7213e1315abfba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                 | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:conv_data_LLM:#examples with triples 2095 (100.0%) out of 2095 examples\n",
      "INFO:conv_data_LLM:#examples with no triples 0 (0.0%) and incomplete triples 0 (0.0%)\n",
      "WARNING:run_LM_task:[SAVE] generation / validation results ➜ /home/xueqiang/MyResearch/experiments/my_works/DialFill_v3/bert_NM_KW_C11_ER8_v31/output_test_seen_KrvMv_m5-m10.jsonl\n",
      "INFO:dialogue_copy_data:mode=mask_post_valid | keywords=2095, keywords_gens=0, keywords_cands=2095, keywords_scores=2095, keywords_is_good=2095, knowledges=0, knowledges_cands=0, knowledges_scores=0, knowledges_is_good=0, masked_pre=2095, masked_post=2095, masked_pre_scores=2095, masked_post_scores=2095\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "process_dict = {\n",
    "    # Step 1\n",
    "    'Ke': ['keyword_extract'],\n",
    "    'Kg': ['keyword_generate'],\n",
    "    'Krv': ['retrieved_keywords_valid'],\n",
    "    'Kgv': ['retrieved_knowledges_valid'],\n",
    "    'Kgrv': ['retrieved_kwd_and_kg_valid'],\n",
    "    'Kagv': ['retrieved_kwd_as_kg_valid'],\n",
    "    # Step 2\n",
    "    'Mg': ['mask_pre_generate', 'mask_post_generate'],\n",
    "    'Mra': ['mask_random_append'],\n",
    "    'Mv': ['mask_pre_valid', 'mask_post_valid'],\n",
    "    # Step 3\n",
    "    'Rf': ['response_filling'],\n",
    "}\n",
    "\n",
    "new_generated_name = []\n",
    "if args.do_generate:\n",
    "    args.retri_triples_num = args.input_triples_num\n",
    "    args.extract_kg_from = args.input_kg_from = 'retrieved_kg'\n",
    "    if os.path.exists(str(args.output_dir) + \"/best_model/lm\"):\n",
    "        best_model_path = str(args.output_dir) + \"/best_model/lm\"\n",
    "    if args.peft_train:\n",
    "        args.peft_pretrain_model = True\n",
    "        args.adapter_name_or_path = best_model_path\n",
    "    else:\n",
    "        args.model_name_or_path = best_model_path\n",
    "\n",
    "    args.add_mask_min_length = mmin = 5\n",
    "    args.add_mask_max_length = mmax = 10\n",
    "\n",
    "    trainer = get_trainer(args)\n",
    "    for num_beams in [1]:\n",
    "        args.num_beams = num_beams\n",
    "        for split in [\n",
    "            'seen',\n",
    "            # 'unseen'\n",
    "        ]:\n",
    "            args.chosen_knowledge_path = f\"{home_path}MyResearch/notebooks/experiments/related_works/GATE/{DATA_NAME}_test_{split}_top10_kg.jsonl\"\n",
    "            args.test_dataset_path = f\"{home_path}MyResearch/data/processed/{DATA_NAME}/Topic_LLM/test_{split}.jsonl\"\n",
    "            for key in [\n",
    "                \"KrvMv\",\n",
    "            ]:\n",
    "                process_list = [\n",
    "                    handler\n",
    "                    for seg in re.findall(r'[A-Z][^A-Z]*', key)\n",
    "                    for handler in process_dict.get(seg, [''])\n",
    "                ]\n",
    "                assert '' not in process_list\n",
    "                args.generation_name = f'output_test_{split}_{key}_m{mmin}-m{mmax}'\n",
    "                new_generated_name.append(args.generation_name)\n",
    "                model = DialogueTransformer(args)\n",
    "                for process in process_list:\n",
    "                    if process in ['keyword_extract', 'mask_random_append']:\n",
    "                        continue\n",
    "                    print(f'{process} is now processing!')\n",
    "                    model.generation_state.mode = process\n",
    "                    trainer.test(model)\n",
    "                    model.generation_state.print_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f507c",
   "metadata": {},
   "source": [
    "# evalation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce556276-45e9-4ae0-b611-b7e35ca08969",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.do_eval:\n",
    "    assert 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DialFill_DM]",
   "language": "python",
   "name": "conda-env-DialFill_DM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
