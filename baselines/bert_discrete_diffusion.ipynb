{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e98a1b",
   "metadata": {},
   "source": [
    "# BERT + Discrete (Masked) Diffusion\n",
    "A minimal training pipeline (LLaDA / SMDM‑style) implemented with **PyTorch Lightning**.\n",
    "\n",
    "*Generated automatically on 2025-08-07 11:26:09.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77193837",
   "metadata": {},
   "source": [
    "## 1  Environment & installs\n",
    "Run the following cell **once** (e.g. on Colab) to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6648c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U torch pytorch-lightning transformers datasets accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a18896e-fb10-4de5-b690-d4b744d84b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 顶格放这段，后面再 import transformers\n",
    "import os, warnings, logging\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"   # HF 4.42+ 支持\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", message=\"The current process just got forked\")  # 可选\n",
    "warnings.filterwarnings(\"ignore\", message=\"A parameter name that contains\")\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)               # 可选\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29254a7b",
   "metadata": {},
   "source": [
    "## 2  Imports / basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa2bd775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import math, random, torch, torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import pytorch_lightning as pl\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, BertForMaskedLM\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    model_name: str = \"bert-base-uncased\"\n",
    "    dataset_name: str = \"wikitext\"\n",
    "    dataset_config: str = \"wikitext-2-raw-v1\"\n",
    "    text_column: str = \"text\"\n",
    "    max_length: int = 128\n",
    "    batch_size: int = 16\n",
    "    num_workers: int = 2\n",
    "    lr: float = 3e-5\n",
    "    weight_decay: float = 0.01\n",
    "    max_steps: int = 3000\n",
    "    warmup_steps: int = 100\n",
    "    val_check_interval: int = 500\n",
    "    log_every_n_steps: int = 20\n",
    "    T: int = 8\n",
    "    mask_token_mode: str = \"mask\"\n",
    "    use_time_embed: bool = False\n",
    "    random_replace_prob: float = 0.0\n",
    "    sampling_topk: int = 50\n",
    "    sampling_temperature: float = 1.0\n",
    "    gradient_clip_val: float = 1.0\n",
    "    precision: str = \"bf16-mixed\" if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else \"16-mixed\"\n",
    "\n",
    "cfg = TrainConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01798252",
   "metadata": {},
   "source": [
    "## 3  Lightning DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45359565",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, cfg: TrainConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name, use_fast=True)\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            self.tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    def prepare_data(self):\n",
    "        load_dataset(self.cfg.dataset_name, self.cfg.dataset_config)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds = load_dataset(self.cfg.dataset_name, self.cfg.dataset_config)\n",
    "        def _preprocess(batch):\n",
    "            txts = [x for x in batch[self.cfg.text_column] if x and not x.isspace()]\n",
    "            enc = self.tokenizer(\n",
    "                txts,\n",
    "                truncation=True,\n",
    "                max_length=self.cfg.max_length,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True\n",
    "            )\n",
    "            return enc\n",
    "        self.train_ds = ds['train'].map(_preprocess, batched=True,\n",
    "                                        remove_columns=ds['train'].column_names)\n",
    "        self.val_ds = ds['validation'].map(_preprocess, batched=True,\n",
    "                                           remove_columns=ds['validation'].column_names)\n",
    "\n",
    "    def collate(self, batch):\n",
    "        input_ids = torch.tensor([x['input_ids'] for x in batch], dtype=torch.long)\n",
    "        attention_mask = torch.tensor([x['attention_mask'] for x in batch], dtype=torch.long)\n",
    "        return {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.train_ds, batch_size=self.cfg.batch_size,\n",
    "                                           shuffle=True, num_workers=self.cfg.num_workers,\n",
    "                                           collate_fn=self.collate, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(self.val_ds, batch_size=self.cfg.batch_size,\n",
    "                                           shuffle=False, num_workers=self.cfg.num_workers,\n",
    "                                           collate_fn=self.collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb92644",
   "metadata": {},
   "source": [
    "## 4  Discrete mask scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0405c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMaskScheduler(torch.nn.Module):\n",
    "    def __init__(self, tokenizer, T: int, max_length: int,\n",
    "                 random_replace_prob: float = 0.0, schedule: str = \"cosine\"):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.mask_id = tokenizer.mask_token_id\n",
    "        self.pad_id  = tokenizer.pad_token_id\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.random_replace_prob = random_replace_prob\n",
    "\n",
    "        ts = torch.arange(1, T + 1, dtype=torch.float)\n",
    "        if schedule == \"linear\":\n",
    "            m = ts / T\n",
    "        else:\n",
    "            m = torch.sin((ts / T) * math.pi / 2.0)\n",
    "\n",
    "        # **把 m_table 注册成 buffer，随模型一起搬到对应 device**\n",
    "        self.register_buffer(\"m_table\", torch.clamp(m, 1e-4, 0.9999))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def q_sample(self, x0_ids: torch.LongTensor, t: torch.LongTensor):\n",
    "        B, L = x0_ids.shape\n",
    "        # m_t = self.m_table[t-1].view(B, 1).to(x0_ids.device)\n",
    "        m_t = self.m_table.to(x0_ids.device)[t.cpu() - 1].view(B, 1)\n",
    "        is_pad = x0_ids.eq(self.pad_id)\n",
    "        mask_draw = torch.rand(B, L, device=x0_ids.device)\n",
    "        to_mask = (mask_draw < m_t) & (~is_pad)\n",
    "        x_t = x0_ids.clone()\n",
    "        x_t[to_mask] = self.mask_id\n",
    "\n",
    "        if self.random_replace_prob > 0:\n",
    "            rnd_draw = torch.rand(B, L, device=x0_ids.device)\n",
    "            do_replace = (rnd_draw < self.random_replace_prob) & (~to_mask) & (~is_pad)\n",
    "            rand_ids = torch.randint(0, self.vocab_size, (B, L), device=x0_ids.device)\n",
    "            rand_ids = torch.where(rand_ids.eq(self.pad_id)|rand_ids.eq(self.mask_id),\n",
    "                                   (rand_ids+1) % self.vocab_size, rand_ids)\n",
    "            x_t[do_replace] = rand_ids[do_replace]\n",
    "        return x_t, to_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f431a69d",
   "metadata": {},
   "source": [
    "## 5  LightningModule (BERT denoiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d64c3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDiscreteDiffusion(pl.LightningModule):\n",
    "    def __init__(self, cfg: TrainConfig, tokenizer):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.cfg = cfg\n",
    "        self.tokenizer = tokenizer\n",
    "        self.bert = BertForMaskedLM.from_pretrained(cfg.model_name)\n",
    "        self.bert.resize_token_embeddings(len(tokenizer))\n",
    "        self.scheduler = DiscreteMaskScheduler(tokenizer, cfg.T, cfg.max_length,\n",
    "                                               cfg.random_replace_prob)\n",
    "        if cfg.use_time_embed:\n",
    "            self.time_embed = torch.nn.Embedding(cfg.T+1, self.bert.config.hidden_size)\n",
    "        else:\n",
    "            self.time_embed = None\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        pg = [\n",
    "            {'params': [p for n,p in self.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': self.cfg.weight_decay},\n",
    "            {'params': [p for n,p in self.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': 0.0},\n",
    "        ]\n",
    "        opt = torch.optim.AdamW(pg, lr=self.cfg.lr)\n",
    "        sched = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=self.cfg.warmup_steps)\n",
    "        return {'optimizer': opt, 'lr_scheduler': {'scheduler': sched, 'interval':'step'}}\n",
    "\n",
    "    def _add_time(self, emb, t):\n",
    "        if self.time_embed is None:\n",
    "            return emb\n",
    "        return emb + self.time_embed(t.clamp(0,self.cfg.T)).unsqueeze(1)\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        x0, attn = batch['input_ids'], batch['attention_mask']\n",
    "        t = torch.randint(1, self.cfg.T+1, (x0.size(0),), device=self.device)\n",
    "        xt, to_mask = self.scheduler.q_sample(x0, t)\n",
    "        if self.time_embed is not None:\n",
    "            emb = self.bert.get_input_embeddings()(xt)\n",
    "            logits = self.bert(inputs_embeds=self._add_time(emb,t),\n",
    "                               attention_mask=attn,return_dict=True).logits\n",
    "        else:\n",
    "            logits = self.bert(input_ids=xt, attention_mask=attn, return_dict=True).logits\n",
    "        loss = F.cross_entropy(logits[to_mask], x0[to_mask]) if to_mask.any() else logits.new_zeros(())\n",
    "        self.log('train/loss', loss, prog_bar=True, on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x0, attn = batch['input_ids'], batch['attention_mask']\n",
    "        t = torch.full((x0.size(0),), math.ceil(self.cfg.T/2), device=self.device)\n",
    "        xt, to_mask = self.scheduler.q_sample(x0, t)\n",
    "        if self.time_embed is not None:\n",
    "            emb = self.bert.get_input_embeddings()(xt)\n",
    "            logits = self.bert(inputs_embeds=self._add_time(emb,t),\n",
    "                               attention_mask=attn,return_dict=True).logits\n",
    "        else:\n",
    "            logits = self.bert(input_ids=xt, attention_mask=attn, return_dict=True).logits\n",
    "        loss = F.cross_entropy(logits[to_mask], x0[to_mask]) if to_mask.any() else logits.new_zeros(())\n",
    "        self.log('val/loss', loss, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, prompts, num_steps=None, start_mask_ratio=1.0):\n",
    "        self.eval()\n",
    "        num_steps = num_steps or self.cfg.T\n",
    "        enc = self.tokenizer(prompts, return_tensors='pt', padding='max_length',\n",
    "                             truncation=True, max_length=self.cfg.max_length).to(self.device)\n",
    "        x = enc.input_ids.clone()\n",
    "        attn = enc.attention_mask\n",
    "        mask_id = self.tokenizer.mask_token_id\n",
    "        # initial masking\n",
    "        rnd = torch.rand_like(x, dtype=torch.float)\n",
    "        mask_init = (rnd < start_mask_ratio) & attn.bool()\n",
    "        x[mask_init] = mask_id\n",
    "        B = x.size(0)\n",
    "        for step in range(num_steps,0,-1):\n",
    "            t = torch.full((B,), step, device=self.device, dtype=torch.long)\n",
    "            if self.time_embed is not None:\n",
    "                emb = self.bert.get_input_embeddings()(x)\n",
    "                logits = self.bert(inputs_embeds=self._add_time(emb,t),\n",
    "                                   attention_mask=attn,return_dict=True).logits\n",
    "            else:\n",
    "                logits = self.bert(input_ids=x, attention_mask=attn, return_dict=True).logits\n",
    "            probs = torch.softmax(logits/self.cfg.sampling_temperature, dim=-1)\n",
    "            topk = min(self.cfg.sampling_topk, probs.size(-1))\n",
    "            if topk < probs.size(-1):\n",
    "                topk_probs, topk_ids = torch.topk(probs, k=topk, dim=-1)\n",
    "                topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)\n",
    "                idx = torch.distributions.Categorical(topk_probs).sample()\n",
    "                sampled = topk_ids.gather(-1, idx.unsqueeze(-1)).squeeze(-1)\n",
    "            else:\n",
    "                sampled = torch.distributions.Categorical(probs).sample()\n",
    "            fill = x.eq(mask_id) & attn.bool()\n",
    "            x[fill] = sampled[fill]\n",
    "        return self.tokenizer.batch_decode(x, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f721e",
   "metadata": {},
   "source": [
    "## 6  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c8a461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xueqiang/anaconda3/envs/DialFill_DM/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "2025-08-07 21:03:16.227110: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-07 21:03:16.242716: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-07 21:03:16.247466: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-07 21:03:16.259133: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-07 21:03:16.959781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af70dfc22be2453b9fcbba11db78a18d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ececd118887492dae6572b2235f6d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type                  | Params\n",
      "----------------------------------------------------\n",
      "0 | bert      | BertForMaskedLM       | 109 M \n",
      "1 | scheduler | DiscreteMaskScheduler | 0     \n",
      "----------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "438.057   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                  | 0/? [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e2cb909f774c00912554c3a91ac0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                         | 0/? [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                       | 0/? [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                       | 0/? [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                       | 0/? [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                       | 0/? [00:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`Trainer.fit` stopped: `max_steps=3000` reached.\n"
     ]
    }
   ],
   "source": [
    "dm = TextDataModule(cfg)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "model = BertDiscreteDiffusion(cfg, dm.tokenizer)\n",
    "trainer = pl.Trainer(max_steps=cfg.max_steps,\n",
    "                     val_check_interval=cfg.val_check_interval,\n",
    "                     gradient_clip_val=cfg.gradient_clip_val,\n",
    "                     precision=cfg.precision,\n",
    "                     accelerator='auto', devices='auto',\n",
    "                     log_every_n_steps=cfg.log_every_n_steps,\n",
    "                     callbacks=[pl.callbacks.ModelCheckpoint(monitor='val/loss', mode='min', save_top_k=1),\n",
    "                                pl.callbacks.LearningRateMonitor(logging_interval='step')])\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b8779",
   "metadata": {},
   "source": [
    "## 7  Sampling demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d784152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: In recent years, diffusion models for language have\n",
      "Output: in recent years, diffusion models in and have\n",
      "\n",
      "Prompt: The quick brown fox\n",
      "Output: 3 of & fox\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"In recent years, diffusion models for language have\",\n",
    "    \"The quick brown fox\"\n",
    "]\n",
    "samples = model.sample(\n",
    "    prompts,\n",
    "    num_steps=cfg.T,\n",
    "    start_mask_ratio=0.5,   # 更少初始掩码\n",
    "    )                       # 在模块里把 topk=10, temperature=0.7\n",
    "\n",
    "for p, s in zip(prompts, samples):\n",
    "    print('\\nPrompt:', p)\n",
    "    print('Output:', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff897f0-b3f0-420e-bc78-0d6125a78c31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DialFill_DM]",
   "language": "python",
   "name": "conda-env-DialFill_DM-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
